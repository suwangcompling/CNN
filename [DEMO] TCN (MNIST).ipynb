{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Convolutional Networks (TCN)\n",
    "\n",
    "* Original work: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (https://arxiv.org/pdf/1803.01271.pdf)\n",
    "\n",
    "\n",
    "<img src=\"FIGS/tcn.png\",width=800,height=800> ![](images/k8s-dashboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 @Jacob Su Wang. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from helpers import checkpoint_model\n",
    "\n",
    "\n",
    "def causal_conv1d(inputs, \n",
    "                  filters,\n",
    "                  kernel_size,\n",
    "                  strides=1, \n",
    "                  dilation_rate=1, \n",
    "                  activation=None, \n",
    "                  use_bias=True, \n",
    "                  kernel_initializer=None, \n",
    "                  bias_initializer=tf.zeros_initializer(), \n",
    "                  kernel_regularizer=None, \n",
    "                  bias_regularizer=None, \n",
    "                  activity_regularizer=None, \n",
    "                  kernel_constraint=None, # seems missing in v1.0.\n",
    "                  bias_constraint=None,   # seems missing in v1.0.\n",
    "                  trainable=True, \n",
    "                  name=None,\n",
    "                  reuse=None, # i added this.\n",
    "                  **kwargs):\n",
    "    \n",
    "    padding = (kernel_size - 1) * dilation_rate\n",
    "        # F(n) = F(n-1) + (kernel_size - 1) * dilation_rate\n",
    "    inputs_padded = tf.pad(inputs, [[0,0],[padding,0],[0,0]])\n",
    "        # pad beginning of input.\n",
    "        # NB: use this rather than tf.pad(.., tf.constant) \n",
    "        #     to have .pad's shape fully specified.\n",
    "    return tf.layers.conv1d(inputs_padded,\n",
    "                            filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=strides,\n",
    "                            padding='valid',\n",
    "                            data_format='channels_last',\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            activation=activation,\n",
    "                            use_bias=use_bias,\n",
    "                            kernel_initializer=kernel_initializer,\n",
    "                            bias_initializer=bias_initializer,\n",
    "                            kernel_regularizer=kernel_regularizer,\n",
    "                            bias_regularizer=bias_regularizer,\n",
    "                            activity_regularizer=activity_regularizer,\n",
    "                            trainable=trainable,\n",
    "                            name=name,\n",
    "                            reuse=reuse)\n",
    "        \n",
    "def temporal_block(inputs, \n",
    "                   filters, \n",
    "                   kernel_size,\n",
    "                   strides,\n",
    "                   dilation_rate,\n",
    "                   conv_block_name,\n",
    "                   rate,\n",
    "                   training=True):\n",
    "    \n",
    "    conv1 = causal_conv1d(inputs, \n",
    "                          filters, \n",
    "                          kernel_size, \n",
    "                          strides, \n",
    "                          dilation_rate,\n",
    "                          activation=tf.nn.relu,\n",
    "                          name=conv_block_name+'-conv1')\n",
    "    conv1_norm = tf.contrib.layers.layer_norm(conv1)\n",
    "    conv1_dropout = tf.layers.dropout(conv1_norm, \n",
    "                                      rate=rate, \n",
    "                                      training=training)\n",
    "    conv2 = causal_conv1d(conv1_dropout,\n",
    "                          filters, \n",
    "                          kernel_size, \n",
    "                          strides, \n",
    "                          dilation_rate,\n",
    "                          activation=tf.nn.relu,\n",
    "                          name=conv_block_name+'-conv2')\n",
    "    conv2_norm = tf.contrib.layers.layer_norm(conv2)\n",
    "    conv2_dropout = tf.layers.dropout(conv2_norm, \n",
    "                                      rate=rate, \n",
    "                                      training=training)\n",
    "    \n",
    "    return tf.nn.relu(conv2_dropout + tf.layers.conv1d(inputs, filters=filters, kernel_size=1))\n",
    "        # residual link: relu(transformed + identity_conv(original))\n",
    "    \n",
    "def temporal_cnn(inputs,\n",
    "                 blocks,\n",
    "                 kernel_size,\n",
    "                 rate,\n",
    "                 block_names):\n",
    "    \n",
    "    outputs = inputs\n",
    "    n_block = len(blocks)\n",
    "    for i in range(n_block):\n",
    "        dilation_rate = 2**i\n",
    "        filters = blocks[i]\n",
    "        outputs = temporal_block(outputs, \n",
    "                                 filters,\n",
    "                                 kernel_size,\n",
    "                                 strides=1,\n",
    "                                 dilation_rate=dilation_rate,\n",
    "                                 conv_block_name=block_names[i],\n",
    "                                 rate=rate)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "class TCN_MNIST:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.blocks = [config['n_filter']]*config['n_level']\n",
    "        self.kernel_size = config['kernel_size']\n",
    "        self.dropout_rate = config['dropout_rate']\n",
    "        self.block_names = config['block_names']\n",
    "        \n",
    "        self.time_steps = 28 * 28\n",
    "        self.n_channel = 1\n",
    "        self.n_class = 10\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        \n",
    "        self.model_dir = config['model_dir']\n",
    "        self.model_name = config['model_name'] \n",
    "        \n",
    "        if config['load_from_saved']:\n",
    "            self.__load_saved_graph()\n",
    "            print('\\nModel loaded for continued training!\\n')\n",
    "        else:\n",
    "            self.__build_new_graph()\n",
    "            print('\\nNew model built for training!\\n')\n",
    "        \n",
    "    def __build_new_graph(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.time_steps, self.n_channel], name='x')\n",
    "            # <batch-size, input-length, num-channels>\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.n_class], name='y')\n",
    "            # <batch-size, num-classes>\n",
    "        \n",
    "        self.logits = tf.layers.dense(\n",
    "            temporal_cnn(inputs=self.x, \n",
    "                         blocks=self.blocks, \n",
    "                         kernel_size=self.kernel_size,\n",
    "                         rate=self.dropout_rate,\n",
    "                         block_names=self.block_names)[:,-1,:], \n",
    "                # temporal_cnn out: <batch-size, output-length, num-filters>\n",
    "                # take the last time-step, retain: <batch-size, num-filters>\n",
    "            self.n_class, # out: <batch-size, num-classes>\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.orthogonal_initializer()\n",
    "        )\n",
    "        self.prediction = tf.nn.softmax(self.logits, name='prediction')\n",
    "        self.correct = tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct, tf.float32), name='accuracy')\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                           labels=self.y), name='loss')\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step, name='train_op')\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def __load_saved_graph(self):\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.import_meta_graph(self.model_dir+self.model_name+'.meta')\n",
    "        self.saver.restore(self.sess, tf.train.latest_checkpoint(self.model_dir))\n",
    "        self.graph = tf.get_default_graph()\n",
    "        \n",
    "        self.x = self.graph.get_tensor_by_name('x:0')\n",
    "        self.y = self.graph.get_tensor_by_name('y:0')\n",
    "        self.prediction = self.graph.get_tensor_by_name('prediction:0')\n",
    "        self.accuracy = self.graph.get_tensor_by_name('accuracy:0')\n",
    "        self.loss = self.graph.get_tensor_by_name('loss:0')\n",
    "        self.global_step = self.graph.get_tensor_by_name('global_step:0')\n",
    "        self.train_op = self.graph.get_tensor_by_name('train_op:0')\n",
    "        \n",
    "\n",
    "def train_mnist_tcn(model_config, train_config):\n",
    "    \n",
    "    print(\"Configuring training ...\\n\")\n",
    "    tcn = TCN_MNIST(model_config)\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = False\n",
    "    batch_size = train_config['batch_size']\n",
    "    verbose = train_config['verbose']\n",
    "    n_epoch = train_config['n_epoch']\n",
    "    n_batch = train_config['n_batch']\n",
    "    acc_threshold = train_config['acc_threshold']\n",
    "    s_dir = tcn.model_dir\n",
    "    s_path = tcn.model_dir+tcn.model_name\n",
    "    \n",
    "    print(\"Load MNIST ...\\n\")\n",
    "    mnist = input_data.read_data_sets(\"/work/04233/sw33286/CNN-STASH/TCN/tcn-mnist/data/\", \n",
    "                                      one_hot=True)\n",
    "    \n",
    "    print(\"Training ...\\n\")\n",
    "    for e in range(n_epoch):\n",
    "        print(\"Epoch {}\".format(e+1))\n",
    "        for _ in range(n_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            batch_x = batch_x.reshape((batch_size,tcn.time_steps,tcn.n_channel))\n",
    "            _, step = tcn.sess.run([tcn.train_op,tcn.global_step], \n",
    "                                   feed_dict={tcn.x:batch_x,tcn.y:batch_y})\n",
    "            if step % verbose == 0:\n",
    "                l, a = tcn.sess.run([tcn.loss,tcn.accuracy],\n",
    "                                    feed_dict={tcn.x:batch_x,tcn.y:batch_y})\n",
    "                test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
    "                test_batch_x = test_batch_x.reshape((batch_size,tcn.time_steps,tcn.n_channel))\n",
    "                test_a = tcn.sess.run(tcn.accuracy, \n",
    "                                      feed_dict={tcn.x:test_batch_x,tcn.y:test_batch_y})\n",
    "                print(\"@step {} | train loss = {:.5f} | train acc = {:.3f} | test acc = {:.3f}\".format(step,\n",
    "                                                                                       l,a,test_a))\n",
    "                if test_a >= acc_threshold:\n",
    "                    checkpoint_model(s_dir, s_path, tcn.saver, tcn.sess)\n",
    "                    print(\"[CHECKPOINT] model saved @step {}, with accuracy {}\".format(step,test_a))\n",
    "        print('\\n')\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model config\n",
    "    parser.add_argument('--n_filter', type=int, default=8)\n",
    "    parser.add_argument('--n_level', type=int, default=6)\n",
    "    parser.add_argument('--kernel_size', type=int, default=8)\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--block_names', type=str, default=\"tcn-\")\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-3)\n",
    "    parser.add_argument('--model_dir', type=str, default=\"/work/04233/sw33286/CNN-STASH/TCN/tcn-mnist/model/\")\n",
    "    parser.add_argument('--model_name', type=str, default=\"mnist-00\")\n",
    "    parser.add_argument('--load_from_saved', type=bool, default=False)\n",
    "    # train config\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--verbose', type=int, default=500)\n",
    "    parser.add_argument('--n_epoch', type=int, default=5)\n",
    "    parser.add_argument('--n_batch', type=int, default=1000)\n",
    "    parser.add_argument('--acc_threshold', type=float, default=0.95)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    block_names = [args.block_names+str(i+1) for i in range(args.n_level)]\n",
    "    model_config = {'n_filter': args.n_filter, 'n_level': args.n_level, 'kernel_size': args.kernel_size, \n",
    "                    'dropout_rate': args.dropout_rate, 'block_names': block_names, \n",
    "                    'learning_rate': args.learning_rate, \n",
    "                    'model_dir': args.model_dir, 'model_name': args.model_name,\n",
    "                    'load_from_saved': args.load_from_saved}\n",
    "    train_config = {'batch_size': args.batch_size, 'verbose': args.verbose, \n",
    "                    'n_epoch': args.n_epoch, 'n_batch': args.n_batch,\n",
    "                    'acc_threshold': args.acc_threshold}\n",
    "    \n",
    "    train_mnist_tcn(model_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEMO] TCN (MNIST).ipynb  [DEVELOPMENT] TCN (MNIST).ipynb  tcn_mnist_demo.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n",
      "Configuring training ...\n",
      "\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n",
      "name: Tesla K40m\n",
      "major: 3 minor: 5 memoryClockRate (GHz) 0.745\n",
      "pciBusID 0000:08:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 11.10GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5\n",
      "\n",
      "New model built for training!\n",
      "\n",
      "Load MNIST ...\n",
      "\n",
      "Extracting /work/04233/sw33286/CNN-STASH/TCN/tcn-mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /work/04233/sw33286/CNN-STASH/TCN/tcn-mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /work/04233/sw33286/CNN-STASH/TCN/tcn-mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /work/04233/sw33286/CNN-STASH/TCN/tcn-mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Training ...\n",
      "\n",
      "Epoch 1\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1752 get requests, put_count=1456 evicted_count=1000 eviction_rate=0.686813 and unsatisfied allocation rate=0.796804\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1752 get requests, put_count=1947 evicted_count=1000 eviction_rate=0.513611 and unsatisfied allocation rate=0.472603\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281\n",
      "@step 500 | train loss = 0.86997 | train acc = 0.734 | test acc = 0.641\n",
      "@step 1000 | train loss = 0.75619 | train acc = 0.719 | test acc = 0.828\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "@step 1500 | train loss = 0.37236 | train acc = 0.906 | test acc = 0.844\n",
      "@step 2000 | train loss = 0.37459 | train acc = 0.891 | test acc = 0.844\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "@step 2500 | train loss = 0.28881 | train acc = 0.906 | test acc = 0.859\n",
      "@step 3000 | train loss = 0.36578 | train acc = 0.891 | test acc = 0.938\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "@step 3500 | train loss = 0.32504 | train acc = 0.906 | test acc = 0.969\n",
      "[CHECKPOINT] model saved @step 3500, with accuracy 0.96875\n",
      "@step 4000 | train loss = 0.20093 | train acc = 0.922 | test acc = 0.938\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "@step 4500 | train loss = 0.50377 | train acc = 0.844 | test acc = 0.953\n",
      "[CHECKPOINT] model saved @step 4500, with accuracy 0.953125\n",
      "@step 5000 | train loss = 0.19862 | train acc = 0.953 | test acc = 0.891\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 tcn_mnist_demo.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
