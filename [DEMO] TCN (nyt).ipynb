{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 @Jacob Su Wang. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/work/04233/sw33286/AIDA-SCRIPTS\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import Indexer, checkpoint_model\n",
    "from itertools import chain, product\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "def causal_conv1d(inputs, \n",
    "                  filters,\n",
    "                  kernel_size,\n",
    "                  strides=1, \n",
    "                  dilation_rate=1, \n",
    "                  activation=None, \n",
    "                  use_bias=True, \n",
    "                  kernel_initializer=None, \n",
    "                  bias_initializer=tf.zeros_initializer(), \n",
    "                  kernel_regularizer=None, \n",
    "                  bias_regularizer=None, \n",
    "                  activity_regularizer=None, \n",
    "                  trainable=True, \n",
    "                  name=None,\n",
    "                  reuse=None, # i added this.\n",
    "                  **kwargs):\n",
    "    \n",
    "    padding = (kernel_size - 1) * dilation_rate\n",
    "    inputs_padded = tf.pad(inputs, [[0,0],[padding,0],[0,0]])\n",
    "        # use this rather than tf.pad(.., tf.constant) \n",
    "        # to have .pad's shape fully specified.\n",
    "    return tf.layers.conv1d(inputs_padded,\n",
    "                            filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=strides,\n",
    "                            padding='valid',\n",
    "                            data_format='channels_last',\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            activation=activation,\n",
    "                            use_bias=use_bias,\n",
    "                            kernel_initializer=kernel_initializer,\n",
    "                            bias_initializer=bias_initializer,\n",
    "                            kernel_regularizer=kernel_regularizer,\n",
    "                            bias_regularizer=bias_regularizer,\n",
    "                            activity_regularizer=activity_regularizer,\n",
    "                            trainable=trainable,\n",
    "                            name=name,\n",
    "                            reuse=reuse)\n",
    "\n",
    "def temporal_block(inputs, \n",
    "                   filters, \n",
    "                   kernel_size,\n",
    "                   strides,\n",
    "                   dilation_rate,\n",
    "                   conv_block_name,\n",
    "                   rate,\n",
    "                   training=True):\n",
    "    \n",
    "    conv1 = causal_conv1d(inputs, \n",
    "                          filters, \n",
    "                          kernel_size, \n",
    "                          strides, \n",
    "                          dilation_rate,\n",
    "                          activation=tf.nn.relu,\n",
    "                          name=conv_block_name+'-conv1')\n",
    "    conv1_norm = tf.contrib.layers.batch_norm(conv1)\n",
    "    conv1_dropout = tf.layers.dropout(conv1_norm, \n",
    "                                      rate=rate, \n",
    "                                      training=training)\n",
    "    conv2 = causal_conv1d(conv1_dropout,\n",
    "                          filters, \n",
    "                          kernel_size, \n",
    "                          strides, \n",
    "                          dilation_rate,\n",
    "                          activation=tf.nn.relu,\n",
    "                          name=conv_block_name+'-conv2')\n",
    "    conv2_norm = tf.contrib.layers.batch_norm(conv2)\n",
    "    conv2_dropout = tf.layers.dropout(conv2_norm, \n",
    "                                      rate=rate, \n",
    "                                      training=training)\n",
    "    \n",
    "    inputs_idconv = tf.layers.conv1d(inputs, filters=filters, kernel_size=1, name=conv_block_name+'-id')\n",
    "    \n",
    "    return tf.nn.relu(conv2_dropout + inputs_idconv)\n",
    "        # residual link: relu(transformed + identity_conv(original))\n",
    "\n",
    "def fully_connected(inputs, \n",
    "                    out_dim,\n",
    "                    activation=None,\n",
    "                    initializer=None,\n",
    "                    name=None):\n",
    "    \n",
    "    return tf.layers.dense(inputs,\n",
    "                           out_dim,\n",
    "                           activation=activation,\n",
    "                           kernel_initializer=initializer,\n",
    "                           name=name)\n",
    "\n",
    "def vdcnn(inputs,\n",
    "          blocks_list,\n",
    "          kernel_size,\n",
    "          rate,\n",
    "          block_names_list):\n",
    "    \n",
    "    outputs = inputs\n",
    "    for blocks,block_names in zip(blocks_list,block_names_list):\n",
    "        n_block = len(blocks)\n",
    "        for i in range(n_block):\n",
    "            dilation_rate = 2**i\n",
    "            filters = blocks[i]\n",
    "            outputs = temporal_block(outputs, \n",
    "                                     filters,\n",
    "                                     kernel_size,\n",
    "                                     strides=1,\n",
    "                                     dilation_rate=dilation_rate,\n",
    "                                     conv_block_name=block_names[i],\n",
    "                                     rate=rate)\n",
    "        outputs = tf.layers.max_pooling1d(outputs, pool_size=2, strides=2)\n",
    "    \n",
    "    fc_dim = int(blocks_list[-1][-1]/2)\n",
    "    for i in range(2):\n",
    "        outputs = fully_connected(outputs, \n",
    "                                  out_dim=fc_dim, \n",
    "                                  activation=tf.nn.relu,\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  name=str(i)) # NB: weight layers must be named to be reused!!\n",
    "    \n",
    "    return outputs[:,-1,:]\n",
    "\n",
    "class PairwiseVDCNN:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.config = config\n",
    "        self.data_dir = self.config['data_dir']\n",
    "        self.info_path = self.config['info_path']\n",
    "        self.FILENAMES = os.listdir(self.data_dir)\n",
    "            # each file is a '.p' filename.\n",
    "        self.indexer, self.word2emb = dill.load(open(self.info_path, 'rb'))\n",
    "            # indexer: Indexer object, word<->index mapping.\n",
    "            # word2emb: word->glove-embedding mapping (300D). \n",
    "        self.model_dir = self.config['model_dir']\n",
    "        self.model_name = self.config['model_name']\n",
    "        self.init_with_glove = self.config['init_with_glove']\n",
    "        \n",
    "        self.load_from_saved = self.config['load_from_saved']\n",
    "        \n",
    "        self.vocab_size = self.config['vocab_size']\n",
    "        self.emb_size = self.config['emb_size']\n",
    "        self.batch_size = self.config['batch_size']\n",
    "        self.max_len = self.config['max_len']\n",
    "        self.kernel_size = self.config['kernel_size']\n",
    "        self.dropout_rate = self.config['dropout_rate']\n",
    "        self.learning_rate = self.config['learning_rate']\n",
    "\n",
    "        if self.init_with_glove:\n",
    "            glove_embs = []\n",
    "            for i in range(len(self.indexer)):\n",
    "                glove_embs.append(self.word2emb[self.indexer.get_object(i)])\n",
    "            self.glove_embs = np.array(glove_embs)\n",
    "        else:\n",
    "            del self.word2emb\n",
    "\n",
    "        if self.load_from_saved:\n",
    "            self.__load_saved_graph()\n",
    "            print('Model loaded for continued training!')\n",
    "        else:\n",
    "            self.__build_new_graph()  \n",
    "            print('New model built for training!')\n",
    "            \n",
    "    def __build_new_graph(self):\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        sess_config = tf.ConfigProto()\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=sess_config)\n",
    "        \n",
    "        self.__init_place_holders()\n",
    "        self.__init_embeddings()\n",
    "        self.__run_vdcnn()\n",
    "        self.__run_score_and_predictions()\n",
    "        self.__run_accuracy()\n",
    "        self.__run_optimization()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()        \n",
    "        \n",
    "    def __init_place_holders(self):    \n",
    "        \n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, self.max_len], name='input_x1') # <bc,mt>\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, self.max_len], name='input_x2')\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name='input_y')\n",
    "    \n",
    "    def __init_embeddings(self):\n",
    "        \n",
    "        with tf.variable_scope('Embeddings'):\n",
    "            self.embeddings = tf.get_variable('embeddings', [self.vocab_size, self.emb_size], \n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "            if self.init_with_glove:\n",
    "                glove_init = self.embeddings.assign(self.glove_embs)\n",
    "            self.input_x1_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_x1) \n",
    "                # <bc,mt,emb>\n",
    "            self.input_x2_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_x2)\n",
    "            \n",
    "    def __run_vdcnn(self):\n",
    "        \n",
    "        with tf.variable_scope('VDCNN') as scope:\n",
    "            self.input_x1_vdcnn = vdcnn(self.input_x1_embedded,\n",
    "                                        blocks_list=[[256,256],[512,512]],\n",
    "                                        kernel_size=self.kernel_size, \n",
    "                                        rate=self.dropout_rate,\n",
    "                                        block_names_list=[['l-256-1','l-256-2'],\n",
    "                                                          ['l-512-1','l-512-2']])\n",
    "                # <bc,out-dim>\n",
    "            scope.reuse_variables()  \n",
    "            self.input_x2_vdcnn = vdcnn(self.input_x2_embedded,\n",
    "                                        blocks_list=[[256,256],[512,512]],\n",
    "                                        kernel_size=self.kernel_size, \n",
    "                                        rate=self.dropout_rate,\n",
    "                                        block_names_list=[['l-256-1','l-256-2'],\n",
    "                                                          ['l-512-1','l-512-2']])        \n",
    "\n",
    "    def __run_score_and_predictions(self):\n",
    "        \n",
    "        W_bi = tf.get_variable('W_bi', [256,256], # final conv layer is halves as per the paper. \n",
    "                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.scores = tf.nn.sigmoid(tf.diag_part(tf.matmul(tf.matmul(self.input_x1_vdcnn,W_bi),\n",
    "                                                      tf.transpose(self.input_x2_vdcnn))),name='scores')\n",
    "        self.predictions = tf.cast(tf.round(self.scores), tf.int32, name='predictions')  \n",
    "        \n",
    "    \n",
    "    def __run_accuracy(self):\n",
    "\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "    def __run_optimization(self):\n",
    "\n",
    "        with tf.name_scope('Loss'):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(self.input_y, tf.float32), \n",
    "                                                             logits=self.scores)\n",
    "            self.loss = tf.reduce_mean(losses, name='loss')  \n",
    "\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step, name='train_op')    \n",
    "    \n",
    "    def __load_saved_graph(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        sess_config = tf.ConfigProto()\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=sess_config)\n",
    "        self.saver = tf.train.import_meta_graph(self.model_dir + self.model_name + '.meta')\n",
    "        self.saver.restore(self.sess, tf.train.latest_checkpoint(self.model_dir))\n",
    "        self.graph = tf.get_default_graph() \n",
    "        \n",
    "        self.input_x1 = self.graph.get_tensor_by_name('input_x1:0')\n",
    "        self.input_x2 = self.graph.get_tensor_by_name('input_x2:0')\n",
    "        self.input_y = self.graph.get_tensor_by_name('input_y:0')\n",
    "        \n",
    "        self.scores = self.graph.get_tensor_by_name('scores:0')\n",
    "        self.predictions = self.graph.get_tensor_by_name('predictions:0')\n",
    "        self.loss = self.graph.get_tensor_by_name('Loss/loss:0')\n",
    "        self.accuracy = self.graph.get_tensor_by_name('Accuracy/accuracy:0')\n",
    "        self.global_step = self.graph.get_tensor_by_name('global_step:0')\n",
    "        self.train_op = self.graph.get_tensor_by_name('train_op:0')\n",
    "        \n",
    "class DataBatcher:\n",
    "    \n",
    "    def __init__(self, data_dir, batch_size, max_len):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __procrustize(self, sent):\n",
    "        \n",
    "        s_len = len(sent)\n",
    "        \n",
    "        return sent[:self.max_len] if s_len>=self.max_len else sent+[0]*(self.max_len-s_len)   \n",
    "    \n",
    "    def get_batch(self, filename):\n",
    "        \n",
    "        doc_a, doc_b, _ = dill.load(open(self.data_dir+filename, 'rb'))\n",
    "        batch_x1, batch_x2, batch_y = [], [], []\n",
    "        ys = [1,0,0,1]\n",
    "        for _ in range(self.batch_size//4): # 4 entries added per iteration.\n",
    "            for i,(da,db) in enumerate(product([doc_a, doc_b], \n",
    "                                               [doc_a, doc_b])):\n",
    "                batch_x1.append(self.__procrustize(random.choice(da)))\n",
    "                batch_x2.append(self.__procrustize(random.choice(db)))\n",
    "                batch_y.append(ys[i])\n",
    "                \n",
    "        return np.array(batch_x1), np.array(batch_x2), np.array(batch_y)  \n",
    "    \n",
    "\n",
    "def train_pairwise_vdcnn(model_config,\n",
    "                         track_dir,\n",
    "                         session_id,\n",
    "                         new_track,\n",
    "                         n_epoch,\n",
    "                         train_size,\n",
    "                         verbose,\n",
    "                         save_freq):\n",
    "    \n",
    "    clf = PairwiseVDCNN(model_config)\n",
    " \n",
    "    dat = DataBatcher(clf.data_dir, clf.batch_size, clf.max_len)\n",
    "    \n",
    "    log_mode = 'w' if new_track else 'a'\n",
    "    with open(track_dir+session_id+'.txt', log_mode) as f:\n",
    "        f.write('\\n\\n== NEW SESSION ==\\n\\n')\n",
    "    loss_track, accuracy_track = [], []\n",
    "    start = time.time()    \n",
    "    try:\n",
    "        for e in range(n_epoch):\n",
    "            with open(track_dir+session_id+'.txt', 'a') as f:\n",
    "                f.write('Epoch '+str(e+1)+'\\n')\n",
    "            file_indices = np.random.choice(list(range(len(clf.FILENAMES))),\n",
    "                                            size=train_size, replace=False)\n",
    "            random.shuffle(file_indices)\n",
    "            curr_loss_track, curr_accuracy_track = [], []\n",
    "            for file_idx in file_indices:\n",
    "                try: # handle bad files show there be any.\n",
    "                    batch_x1,batch_x2,batch_y = dat.get_batch(clf.FILENAMES[file_idx])\n",
    "                except:\n",
    "                    continue\n",
    "                fd = {clf.input_x1:batch_x1, clf.input_x2:batch_x2, clf.input_y:batch_y}\n",
    "                _,step,loss_,accuracy_ = clf.sess.run([clf.train_op,clf.global_step,\n",
    "                                                       clf.loss,clf.accuracy], feed_dict=fd)\n",
    "                curr_loss_track.append(loss_)\n",
    "                curr_accuracy_track.append(accuracy_)\n",
    "                if step % save_freq == 0:\n",
    "                    checkpoint_model(clf.model_dir, clf.model_dir+clf.model_name,\n",
    "                                     clf.saver, clf.sess)\n",
    "                if step % verbose == 0:\n",
    "                    with open(track_dir+session_id+'.txt', 'a') as f:\n",
    "                        avg_loss = np.mean(curr_loss_track)\n",
    "                        avg_acc = np.mean(curr_accuracy_track)\n",
    "                        loss_track.append(avg_loss)\n",
    "                        accuracy_track.append(avg_acc)\n",
    "                        f.write('loss & accuracy at step {}: <{:.5f}, {:.2f}> (time elapsed = {:.2f} secs)\\n'.format(step, \n",
    "                                                                                        avg_loss,\n",
    "                                                                                        avg_acc,\n",
    "                                                                                        time.time()-start,2))\n",
    "                    start = time.time()\n",
    "                    curr_loss_track, curr_accuracy_track = [], []\n",
    "        with open(track_dir+session_id+'-final.txt', log_mode) as f:\n",
    "            f.write('final avg loss & accuracy: <{:.5f}, {:.5f}>'.format(np.mean(loss_track),\n",
    "                                                                         np.mean(accuracy_track)))\n",
    "    except KeyboardInterrupt:\n",
    "        print('Stopped!')   \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model config\n",
    "    parser.add_argument('--data_dir', type=str, default=\"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_code/\")\n",
    "    parser.add_argument('--info_path', type=str, default=\"/work/04233/sw33286/AIDA-DATA/nyt_eng_salads_info/indexer_word2emb_100k.p\")\n",
    "    parser.add_argument('--model_dir', type=str, default=\"/work/04233/sw33286/AIDA-CNN-MODEL-SAVE/our-model-no-context/\")\n",
    "    parser.add_argument('--model_name', type=str, default=\"vdcnn-no-context\")\n",
    "    parser.add_argument('--init_with_glove', type=bool, default=True) \n",
    "    parser.add_argument('--load_from_saved', type=bool, default=False)\n",
    "    parser.add_argument('--vocab_size', type=int, default=100001)\n",
    "    parser.add_argument('--emb_size', type=int, default=300)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--max_len', type=int, default=32)\n",
    "    parser.add_argument('--kernel_size', type=int, default=3)\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.2)\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-5)\n",
    "    # train config\n",
    "    parser.add_argument('--track_dir', type=str, default=\"/work/04233/sw33286/AIDA-TRACKS/sentence-tracks-cnn/no-context/\")\n",
    "    parser.add_argument('--session_id', type=str, default='0000')\n",
    "    parser.add_argument('--new_track', type=bool, default=True)\n",
    "    parser.add_argument('--n_epoch', type=int, default=1)\n",
    "    parser.add_argument('--train_size', type=int, default=10)\n",
    "    parser.add_argument('--verbose', type=int, default=1)\n",
    "    parser.add_argument('--save_freq', type=int, default=5)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    config = {'data_dir':args.data_dir, 'info_path':args.info_path, \n",
    "              'model_dir':args.model_dir, 'model_name':args.model_name,\n",
    "              'init_with_glove':args.init_with_glove, 'load_from_saved':args.load_from_saved,\n",
    "              'vocab_size':args.vocab_size, 'emb_size':args.emb_size,\n",
    "              'batch_size':args.batch_size, 'max_len':args.max_len,\n",
    "              'kernel_size':args.kernel_size, 'dropout_rate':args.dropout_rate,\n",
    "              'learning_rate':args.learning_rate}\n",
    "    \n",
    "    train_pairwise_vdcnn(model_config=config,\n",
    "                         track_dir=args.track_dir,\n",
    "                         session_id=args.session_id,\n",
    "                         new_track=args.new_track,\n",
    "                         n_epoch=args.n_epoch,\n",
    "                         train_size=args.train_size,\n",
    "                         verbose=args.verbose,\n",
    "                         save_freq=args.save_freq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/04233/sw33286/DEV-CNN\n",
      "[DEMO] TCN (MNIST).ipynb\t FIGS\n",
      "[DEMO] TCN (nyt).ipynb\t\t pairwise_vdcnn.py\n",
      "[DEVELOPMENT] TCN (MNIST).ipynb  tcn_mnist_demo.py\n",
      "[DEVELOPMENT] TCN (nyt).ipynb\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n",
      "name: Tesla K40m\n",
      "major: 3 minor: 5 memoryClockRate (GHz) 0.745\n",
      "pciBusID 0000:08:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 6.96GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5\n",
      "New model built for training!\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1940 get requests, put_count=1927 evicted_count=1000 eviction_rate=0.518941 and unsatisfied allocation rate=0.573711\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\n"
     ]
    }
   ],
   "source": [
    "# train new model\n",
    "\n",
    "!python3 pairwise_vdcnn.py --train_size=10 --verbose 1 --save_freq 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n",
      "name: Tesla K40m\n",
      "major: 3 minor: 5 memoryClockRate (GHz) 0.745\n",
      "pciBusID 0000:08:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 6.96GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n",
      "I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\n",
      "I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:\n",
      "I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): Tesla K40m, Compute Capability 3.5\n",
      "Model loaded for continued training!\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1940 get requests, put_count=1926 evicted_count=1000 eviction_rate=0.519211 and unsatisfied allocation rate=0.574227\n",
      "I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\n"
     ]
    }
   ],
   "source": [
    "# load from saved model\n",
    "\n",
    "!python3 pairwise_vdcnn.py --train_size=10 --verbose 1 --save_freq 5 --load_from_saved 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
